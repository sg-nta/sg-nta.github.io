---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - "/wordpress/"
  - "/wordpress/index.html"
---

{% include base_path %}

   
Welcome to my homepage! I'm An Nguyen The (in Vietnamese: Nguyễn Thế An), and I am currently a research resident at [FPT Software AI Center](https://fpt-aicenter.com/en/) where I am fortunate to be advised by Professor [Tan Nguyen](https://tanmnguyen89.github.io/) and Dr [Thieu Vo](https://scholar.google.at/citations?user=CM2qJSoAAAAJ&hl=en/). Before that, I graduated as a valedictorian from [Hanoi University of Science and Technology](https://hust.edu.vn/) with a Bachelor's degree in Data Science and Artificial Intelligence. 

## Research Interests 
My current research centers on fundamentals of State space models and Equivariant models. I am also open to diversifying my research to various aspects in the future.


<span style="color:red"> **(\*) denotes equal contribution.** </span> <br/>
## Selected Preprints
### Mixture of Experts Meets Prompt-Based Continual Learning
*Minh Le, __An Nguyen__\*, Huy Nguyen\*, Trang Nguyen\*, Trang Pham\*, Linh Van Ngo, Nhat Ho*<br/>
Advances in NeurIPS, 2024 [[arXiv](https://arxiv.org/abs/2405.14124)]
### Monomial Matrix Group Equivariant Neural Functional Networks
*Hoang V. Tran\*, Thieu N. Vo\*, Tho H. Tran, __An T. Nguyen__, Tan Minh Nguyen<br/>
Advances in NeurIPS, 2024 [[arXiv](https://arxiv.org/abs/2409.11697)]
### Equivariant Neural Functional Networks for Transformers
*Hoang V. Tran\*, Thieu N. Vo\*, __An T. Nguyen__\*, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, Tan Minh Nguyen<br/>
Under review, [[arXiv](https://arxiv.org/abs/2410.04209)]
### Equivariant Polynomial Functional Networks
*Thieu N. Vo\*, Hoang V. Tran\*, Tho Tran Huu, __An T. Nguyen__, Thanh Tran, Minh-Khoi Nguyen-Nhat, Duy-Tung Pham, Tan Minh Nguyen<br/>
Under review, [[arXiv](https://arxiv.org/abs/2410.04213)]



<!-- ## Selected Publications on Mixture of Experts
### On Least Squares Estimation in Softmax Gating Mixture of Experts
*__Huy Nguyen__, Nhat Ho, Alessandro Rinaldo*<br/>
Proceedings of the ICML, 2024. [[arXiv](https://arxiv.org/abs/2402.02952)]
### Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?
*__Huy Nguyen__, Pedram Akbarian, Nhat Ho*<br/>
Proceedings of the ICML, 2024. [[arXiv](https://arxiv.org/abs/2401.13875)]
### Demystifying Softmax Gating Function in Gaussian Mixture of Experts 
*__Huy Nguyen__, TrungTin Nguyen, Nhat Ho*<br/>
Advances in NeurIPS, 2023  <span style="color:red"> **(Spotlight)** </span>. [[arXiv](https://arxiv.org/abs/2305.03288)] [[NeurIPS](https://openreview.net/pdf?id=cto6jIIbMZ)]
### Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts
*__Huy Nguyen__, Pedram Akbarian, Fanqi Yan, Nhat Ho*<br/>
Proceedings of the ICLR, 2024. [[arXiv](https://arxiv.org/abs/2309.13850)]
### A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
*__Huy Nguyen__, Pedram Akbarian, TrungTin Nguyen, Nhat Ho*<br/>
Proceedings of the ICML, 2024. [[arXiv](https://arxiv.org/abs/2310.14188)]
### Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts
*__Huy Nguyen\*__, TrungTin Nguyen\*, Khai Nguyen, Nhat Ho*<br/>
In AISTATS, 2024. [[arXiv](https://arxiv.org/abs/2305.07572)]
### On Parameter Estimation in Deviated Gaussian Mixture of Experts
*__Huy Nguyen__, Khai Nguyen, Nhat Ho*<br/>
In AISTATS, 2024. [[arXiv](https://arxiv.org/abs/2402.05220)]

## Selected Publications on Optimal Transport
### Entropic Gromov-Wasserstein between Gaussian Distributions
*__Huy Nguyen\*__, Khang Le\*, Dung Le\*, Dat Do, Tung Pham, Nhat Ho*<br/>
Proceedings of the ICML, 2022.  [[arXiv](https://arxiv.org/abs/2108.10961)] [[ICML](https://proceedings.mlr.press/v162/le22a.html)]
### On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity
*__Huy Nguyen\*__, Khang Le\*, Khai Nguyen, Tung Pham, Nhat Ho*<br/>
In AISTATS, 2022.  [[arXiv](https://arxiv.org/abs/2108.07992)] [[AISTATS](https://proceedings.mlr.press/v151/le22a.html)]
### On Robust Optimal Transport: Computational Complexity and Barycenter Computation
*__Huy Nguyen\*__, Khang Le\*, Quang Minh Nguyen, Tung Pham, Hung Bui, Nhat Ho*<br/>
Advances in NeurIPS, 2021.  [[arXiv](https://arxiv.org/abs/2102.06857)] [[NeurIPS](https://proceedings.neurips.cc/paper/2021/hash/b80ba73857eed2a36dc7640e2310055a-Abstract.html)]



## Recent News
- **[May 2024]** Three new papers on Mixture of Experts [[1](https://arxiv.org/abs/2405.13997)], [[2](https://arxiv.org/abs/2405.14131)] and [[3](https://arxiv.org/abs/2405.14124)] are out!
- **[May 2024]** Three papers on Mixture of Experts, [[1](https://arxiv.org/abs/2402.02952)], [[2](https://arxiv.org/abs/2401.13875)] and [[3](https://arxiv.org/abs/2310.14188)], are accepted to ICML 2024.
- **[Apr 2024]** I was offered the AISTATS 2024 registration grant. See you in Valencia, Spain this May!
- **[Mar 2024]** I received the ICLR 2024 Travel Award. See you in Vienna, Austria this May!
- **[Feb 2024]** Two new papers on the applications of Mixture of Experts in Medical Images [[1](https://arxiv.org/abs/2402.03226)] and Large Language Models [[2](https://arxiv.org/abs/2402.02526)] are out!
- **[Feb 2024]** Two new papers on the theory of Mixture of Experts, [[1](https://arxiv.org/abs/2402.02952)] and [[2](https://arxiv.org/abs/2401.13875)], are out! 
- **[Jan 2024]** Two papers on Mixture of Experts, [[1](https://arxiv.org/abs/2305.07572)] and [[2](https://arxiv.org/abs/2402.05220)], are accepted to AISTATS 2024.
- **[Jan 2024]** Our paper "[Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://arxiv.org/abs/2309.13850)" is accepted to ICLR 2024.
- **[Dec 2023]** Our paper "[Fast Approximation of the Generalized Sliced-Wasserstein Distance](https://openreview.net/forum?id=u3JeFO8G8s)" is accepted to ICASSP 2024.
- **[Oct 2023]** I received the NeurIPS 2023 Scholar Award. See you in New Orleans this December!
- **[Oct 2023]** Our new paper "[A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts](https://arxiv.org/pdf/2310.14188.pdf)" is out.
- **[Sep 2023]** Our new paper "[Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://arxiv.org/pdf/2309.13850.pdf)" is out.
- **[Sep 2023]** We have two papers accepted to NeurIPS 2023, [[1](https://arxiv.org/pdf/2305.03288.pdf)] as <span style="color:red"> **spotlight** </span> and [[2](https://arxiv.org/pdf/2301.11808.pdf)] as poster.
- **[Jul 2023]** We will present the paper "[Fast Approximation of the Generalized Sliced-Wasserstein Distance](https://openreview.net/pdf?id=u3JeFO8G8s)" at the Frontier4LCD workshop, ICML 2023.
- **[May 2023]** Three new papers on the Mixture of Experts theory are out! See more at [[1]](https://arxiv.org/abs/2305.03288), [[2]](https://arxiv.org/abs/2305.07572) and [[3](https://huynm99.github.io/Deviated_MoE.pdf)].
- **[Feb 2023]** Our new paper on Mixture Models theory "[Minimax Optimal Rate for Parameter Estimation in Multivariate Deviated Models](https://arxiv.org/abs/2301.11808)" is out.

## Professional Services
- Conference Reviewer: ICML (2022,2024), NeurIPS (2022-2024), AISTATS (2022-2024) and ICLR (2024).
- Journal Reviewer: Electronic Journal of Statistics
- Workshop Reviewer: Frontier4LCD (ICML 2023). -->
